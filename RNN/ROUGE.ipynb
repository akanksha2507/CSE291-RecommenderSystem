{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually using Rougue, install it through this link:\n",
    "\n",
    "git clone https://github.com/tagucci/pythonrouge.git\n",
    "python setup.py install\n",
    "\n",
    "\n",
    "and for other references/examples check this repo out:\n",
    "https://github.com/tagucci/pythonrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pythonrouge.pythonrouge import Pythonrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tip = [[\"get enough enough here to dinner. to to early get to\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ground_truth = [[[\"Get here early enough to have dinner\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary_file_exist = False if we are using lists as above\n",
    "# recall_only = True if we want the recall scores\n",
    "# change number of n_gram to change n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexRank ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex_rank_generation = []\n",
    "for line in open('./Result_Lex.txt'):\n",
    "    lex_rank_generation.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lex_rank_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Scores = []\n",
    "\n",
    "for i in range(len(lex_rank_generation)):\n",
    "    lex_summ,tip = lex_rank_generation[i].split(\"\\t\")\n",
    "    hypothesis = lex_summ\n",
    "    reference = tip \n",
    "    rouge = Rouge()\n",
    "    score = rouge.get_scores(reference, hypothesis)\n",
    "    Scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': 0.11764705484044613, 'p': 0.21428571428571427, 'r': 0.08108108108108109}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores[1][0]['rouge-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-l': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'rouge-2': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'rouge-1': {'p': 0.0, 'r': 0.0, 'f': 0.0}}]\n"
     ]
    }
   ],
   "source": [
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating averages\n",
    "\n",
    "p1,r1,f1,p2,r2,f2 = [],[],[],[],[],[]\n",
    "\n",
    "for score in Scores:\n",
    "    score = score[0]\n",
    "    p1.append(score['rouge-1']['p'])\n",
    "    r1.append(score['rouge-1']['r'])\n",
    "    f1.append(score['rouge-1']['f'])\n",
    "    \n",
    "    p2.append(score['rouge-2']['p'])\n",
    "    r2.append(score['rouge-2']['r'])\n",
    "    f2.append(score['rouge-2']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_scores_lex = {}\n",
    "\n",
    "rouge_scores_lex['rouge_1'] = {}\n",
    "rouge_scores_lex['rouge_2'] = {}\n",
    "\n",
    "rouge_scores_lex['rouge_1']['p'] = np.mean(p1)\n",
    "rouge_scores_lex['rouge_1']['r'] = np.mean(r1)\n",
    "rouge_scores_lex['rouge_1']['f'] = np.mean(f1)\n",
    "\n",
    "rouge_scores_lex['rouge_2']['p'] = np.mean(p2)\n",
    "rouge_scores_lex['rouge_2']['r'] = np.mean(r2)\n",
    "rouge_scores_lex['rouge_2']['f'] = np.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_1': {'f': 0.045617267706940579,\n",
       "  'p': 0.069400064933161615,\n",
       "  'r': 0.045160131620635147},\n",
       " 'rouge_2': {'f': 0.0014726422229482576,\n",
       "  'p': 0.0025476265181880856,\n",
       "  'r': 0.0013569445586258512}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_1': {'f': 0.045617267706940579,\n",
       "  'p': 0.069400064933161615,\n",
       "  'r': 0.045160131620635147},\n",
       " 'rouge_2': {'f': 0.0014726422229482576,\n",
       "  'p': 0.0025476265181880856,\n",
       "  'r': 0.0013569445586258512}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RNN_scores = []\n",
    "rnn_file = open('./Result_RNN.txt')\n",
    "for line in rnn_file:\n",
    "    result,tip = line.split(\"\\t\")\n",
    "    rouge = Rouge()\n",
    "    reference, hypothesis = repr(tip), repr(result)\n",
    "    score = rouge.get_scores(reference, hypothesis)\n",
    "    RNN_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.09523809024943337, 'p': 0.1, 'r': 0.09090909090909091},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.09480909480869641, 'p': 0.1, 'r': 0.09090909090909091}}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_scores[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating averages\n",
    "\n",
    "p1,r1,f1,p2,r2,f2 = [],[],[],[],[],[]\n",
    "\n",
    "for score in RNN_scores:\n",
    "    score = score[0]\n",
    "    p1.append(score['rouge-1']['p'])\n",
    "    r1.append(score['rouge-1']['r'])\n",
    "    f1.append(score['rouge-1']['f'])\n",
    "    \n",
    "    p2.append(score['rouge-2']['p'])\n",
    "    r2.append(score['rouge-2']['r'])\n",
    "    f2.append(score['rouge-2']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_scores_rnn = {}\n",
    "\n",
    "rouge_scores_rnn['rouge_1'] = {}\n",
    "rouge_scores_rnn['rouge_2'] = {}\n",
    "\n",
    "rouge_scores_rnn['rouge_1']['p'] = np.mean(p1)\n",
    "rouge_scores_rnn['rouge_1']['r'] = np.mean(r1)\n",
    "rouge_scores_rnn['rouge_1']['f'] = np.mean(f1)\n",
    "\n",
    "rouge_scores_rnn['rouge_2']['p'] = np.mean(p2)\n",
    "rouge_scores_rnn['rouge_2']['r'] = np.mean(r2)\n",
    "rouge_scores_rnn['rouge_2']['f'] = np.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_1': {'f': 0.023937933639444978,\n",
       "  'p': 0.02422948923042521,\n",
       "  'r': 0.029232954545454548},\n",
       " 'rouge_2': {'f': 0.00023778660846403478,\n",
       "  'p': 0.00030337627298981609,\n",
       "  'r': 0.0003055555555555556}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_1': {'f': 0.045617267706940579,\n",
       "  'p': 0.069400064933161615,\n",
       "  'r': 0.045160131620635147},\n",
       " 'rouge_2': {'f': 0.0014726422229482576,\n",
       "  'p': 0.0025476265181880856,\n",
       "  'r': 0.0013569445586258512}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RNN_scores = []\n",
    "i = 0\n",
    "for d in lex_rank_generation:\n",
    "    lex_summ,tip = d.split(\"\\t\")\n",
    "    i += 1\n",
    "    if i %20 == 0:\n",
    "        print i,\n",
    "    rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary= [lex_summ], reference=[tip],\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=True, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=True, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "    score = rouge.calc_score()\n",
    "    \n",
    "    scores.append(score)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROUGE-2-F': 0.0, 'ROUGE-SU4-R': 0.0, 'ROUGE-1-F': 0.0, 'ROUGE-SU4-F': 0.0, 'ROUGE-2-R': 0.0, 'ROUGE-1-R': 0.0}\n"
     ]
    }
   ],
   "source": [
    "lex_summ,tip = lex_rank_generation[0].strip().split(\"\\t\")\n",
    "rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary= [[lex_summ]], reference=[[tip]],\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=False, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=False, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "score = rouge.calc_score()\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lex_summ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-28b84accf303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using default configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m rouge = Pythonrouge(summary_file_exist=False,\n\u001b[0;32m----> 3\u001b[0;31m                     \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlex_summ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mn_gram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROUGE_SU4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROUGE_L\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mrecall_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lex_summ' is not defined"
     ]
    }
   ],
   "source": [
    "# Using default configuration\n",
    "rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary= lex_summ, reference=tip,\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=True, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=True, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('tips_lexrank.txt','r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_tips=[]\n",
    "lex_summary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in file:\n",
    "    l=l.split('\\t\\t')\n",
    "    tip=l[0].strip()\n",
    "    lex_summ=l[1].strip()\n",
    "    real_tips+=[tip]\n",
    "    lex_summary+=[lex_summ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = rouge.calc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50168, 50168)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_tips), len(lex_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rouge_score=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(real_tips)):\n",
    "    lex_summ=lex_summary[i]\n",
    "    tip=real_tips[i]\n",
    "    score = rouge.calc_score()\n",
    "    rouge_score.append(score)\n",
    "    if i%1000==0:\n",
    "        print i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1': 1.0, 'ROUGE-2': 0.0, 'ROUGE-SU4': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to save the tips and abstracts in terms of text files and then use it, follow next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file directory should look like this:\n",
    "    \n",
    "    # Directory format sample\n",
    "1 system summary and 4 reference summaries.\n",
    "- system summary\n",
    "./summary_path/summaryA.txt\n",
    "\n",
    "- reference summary\n",
    "./reference_path/summaryA.1.txt\n",
    "./reference_path/summaryA.2.txt\n",
    "./reference_path/summaryA.3.txt\n",
    "./reference_path/summaryA.4.txt\n",
    "\n",
    "File name of reference summaries should be same as the system summary.\n",
    "In this case, system file is \"summaryA.txt\" and reference files should have \"summaryA\" in file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tip = './sample/tips/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ground_truths = './sample/ground_truths/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROUGE_dir = './pythonrouge/RELEASE-1.5.5/ROUGE-1.5.5.pl' # Will be in the installed directory. change path to that\n",
    "# data_dir = './pythonrouge/RELEASE-1.5.5/data/' # Need to add this data too. So change this directory as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rouge_files = Pythonrouge(summary_file_exist=True,\n",
    "#                         peer_path=tip, model_path=ground_truths,\n",
    "#                         n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "#                         recall_only=True,\n",
    "#                         stemming=True, stopwords=True,\n",
    "#                         word_level=True, length_limit=True, length=50,\n",
    "#                         use_cf=False, cf=95, scoring_formula='average',\n",
    "#                         resampling=True, samples=1000, favor=True, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score = rouge_files.calc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
