{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_epochs = 1\n",
    "# total_series_length = 50000\n",
    "# truncated_backprop_length = 15\n",
    "# state_size = 4\n",
    "# num_classes = 2\n",
    "# echo_step = 3\n",
    "# batch_size = 5\n",
    "# num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "\n",
    "# def generateData():\n",
    "#     x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "#     y = np.roll(x, echo_step)\n",
    "#     y[0:echo_step] = 0\n",
    "\n",
    "#     x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "#     y = y.reshape((batch_size, -1))\n",
    "\n",
    "#     return (x, y)\n",
    "\n",
    "# batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "# batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "# init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "\n",
    "# W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "# b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "# W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "# b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# # Unpack columns\n",
    "# inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "# labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "# # Forward pass\n",
    "# current_state = init_state\n",
    "# states_series = []\n",
    "# for current_input in inputs_series:\n",
    "#     current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "#     input_and_state_concatenated = tf.concat([current_input, current_state],1)  # Increasing number of columns\n",
    "\n",
    "#     next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "#     states_series.append(next_state)\n",
    "#     current_state = next_state\n",
    "\n",
    "# logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "# predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "# losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "# total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "# train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "\n",
    "# def plot(loss_list, predictions_series, batchX, batchY):\n",
    "#     plt.subplot(2, 3, 1)\n",
    "#     plt.cla()\n",
    "#     plt.plot(loss_list)\n",
    "\n",
    "#     for batch_series_idx in range(5):\n",
    "#         one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "#         single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "#         plt.subplot(2, 3, batch_series_idx + 2)\n",
    "#         plt.cla()\n",
    "#         plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "#         left_offset = range(truncated_backprop_length)\n",
    "#         plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "#         plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "#         plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "#     plt.draw()\n",
    "#     plt.pause(0.0001)\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.initialize_all_variables())\n",
    "#     plt.ion()\n",
    "#     plt.figure()\n",
    "#     plt.show()\n",
    "#     loss_list = []\n",
    "\n",
    "#     for epoch_idx in range(num_epochs):\n",
    "#         x,y = generateData()\n",
    "#         _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "#         print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "#         for batch_idx in range(num_batches):\n",
    "#             start_idx = batch_idx * truncated_backprop_length\n",
    "#             end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "#             batchX = x[:,start_idx:end_idx]\n",
    "#             batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "#             _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "#                 [total_loss, train_step, current_state, predictions_series],\n",
    "#                 feed_dict={\n",
    "#                     batchX_placeholder:batchX,\n",
    "#                     batchY_placeholder:batchY,\n",
    "#                     init_state:_current_state\n",
    "#                 })\n",
    "\n",
    "#             loss_list.append(_total_loss)\n",
    "\n",
    "#             if batch_idx%100 == 0:\n",
    "#                 print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "#                 plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# total_series_length = 20\n",
    "# echo_step = 1\n",
    "# x = np.array(np.random.choice(5,10))\n",
    "# y = np.roll(x, echo_step)\n",
    "# y[0:echo_step] = 0\n",
    "\n",
    "# x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "# y = y.reshape((batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy\n",
    "def generate_vocabulary():\n",
    "    i = 0\n",
    "    vocab = set() \n",
    "    with open(\"tip.json\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tip = json.loads(line.strip())['text']\n",
    "#             print (tip)\n",
    "            words = tip.lower().split()\n",
    "            vocab |= set(words)\n",
    "            i +=1\n",
    "            if i == 5000:\n",
    "                break\n",
    "    vocab.add('start')\n",
    "    \n",
    "    return vocab,list(vocab)\n",
    "# def generate_data():\n",
    "vocab_set,vocab_list = generate_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {vocab_list[i]:i for i in range(len(vocab_list))}\n",
    "transnet_size = 100\n",
    "sequence_length = 2\n",
    "def one_hot(word):\n",
    "    x = [0]*len(vocab_list)\n",
    "    if word in word_index:\n",
    "        x[word_index[word]] = 1\n",
    "    return x\n",
    "def one_hot_(k,l):\n",
    "    x = [0]*(l)\n",
    "    while k > 0:        \n",
    "        x[k%l] = 1\n",
    "        k = k//l\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def generate_data(train=True):\n",
    "#     j = 0\n",
    "#     with open(\"tip.json\") as f:\n",
    "        \n",
    "#         X = []\n",
    "#         Y = []\n",
    "#         for line in f:\n",
    "#             if not train:\n",
    "#                 if j == 0:\n",
    "#                     j += 1\n",
    "#                     continue\n",
    "#             tip = json.loads(line.strip())['text']\n",
    "#             print(tip)\n",
    "# #             tip =  + ti[]\n",
    "# #             vec = numpy.random.random_sample(transnet_size).tolist()\n",
    "#             vec = one_hot_(j,transnet_size)\n",
    "#             words = ['start' for _ in range(sequence_length)] + tip.strip().lower().split()\n",
    "#             vectors = []\n",
    "#             onehot = []\n",
    "# #             words = words[:4]\n",
    "#             for word in words:\n",
    "#                 encoding = one_hot(word)\n",
    "#                 onehot += [encoding]\n",
    "#                 vectors += [encoding+vec]\n",
    "#             for i in range(sequence_length-1,len(words)-1):\n",
    "# #                 print()\n",
    "#                 X += [vectors[i-sequence_length+1:i+1]]\n",
    "#                 Y += [onehot[i+1]]\n",
    "# #                 print(word,end=\" \")\n",
    "# #             print()\n",
    "            \n",
    "#             j += 1\n",
    "#             if train and j == 2:\n",
    "                \n",
    "#                 X = numpy.array(X)\n",
    "#                 Y = numpy.array(Y)\n",
    "#     #             print(X)\n",
    "#                 print(Y.shape)\n",
    "#                 print(X.shape)\n",
    "#                 return  X,Y\n",
    "#             elif not train and j == 2:\n",
    "                \n",
    "#                 X = numpy.array(X)\n",
    "#                 Y = numpy.array(Y)\n",
    "#     #             print(X)\n",
    "#                 print(Y.shape)\n",
    "#                 print(X.shape)\n",
    "#                 return X,Y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_train():\n",
    "    j = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    total = 1000\n",
    "    with open(\"tip.json\") as f:\n",
    "        for line in f:\n",
    "            tip = json.loads(line.strip())['text']\n",
    "            vec = one_hot_(j,transnet_size)\n",
    "            words = ['start' for _ in range(sequence_length)] + tip.strip().lower().split()\n",
    "            vectors = []\n",
    "            onehot = []\n",
    "            for word in words:\n",
    "                encoding = one_hot(word)\n",
    "                onehot += [encoding]\n",
    "                vectors += [encoding+vec]\n",
    "            for i in range(sequence_length-1,len(words)-1):\n",
    "                X += [vectors[i-sequence_length+1:i+1]]\n",
    "                Y += [onehot[i+1]]\n",
    "            j += 1\n",
    "            if j % 100 == 0:\n",
    "                print(j,end=\" \")\n",
    "            if j == total:\n",
    "                break\n",
    "    return numpy.array(X),numpy.array(Y)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y =generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab_list)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,input_shape=(sequence_length,V+transnet_size)))\n",
    "model.add(Dense(V,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12541"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 200 300 400 500 600 700 800 900 1000 "
     ]
    }
   ],
   "source": [
    "X_train, Y_train = generate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9471/9471 [==============================] - 53s - loss: 7.7952 - acc: 0.0374    \n",
      "Epoch 2/10\n",
      "9471/9471 [==============================] - 25s - loss: 7.1114 - acc: 0.0454    \n",
      "Epoch 3/10\n",
      "9471/9471 [==============================] - 25s - loss: 6.8436 - acc: 0.0511    \n",
      "Epoch 4/10\n",
      "9471/9471 [==============================] - 25s - loss: 6.6923 - acc: 0.0658    \n",
      "Epoch 5/10\n",
      "9471/9471 [==============================] - 25s - loss: 6.6237 - acc: 0.0866    \n",
      "Epoch 6/10\n",
      "9471/9471 [==============================] - 24s - loss: 6.7739 - acc: 0.1046    - \n",
      "Epoch 7/10\n",
      "9471/9471 [==============================] - 24s - loss: 7.0537 - acc: 0.1225    \n",
      "Epoch 8/10\n",
      "9471/9471 [==============================] - 25s - loss: 7.2325 - acc: 0.1464    \n",
      "Epoch 9/10\n",
      "9471/9471 [==============================] - 24s - loss: 7.2896 - acc: 0.1640    \n",
      "Epoch 10/10\n",
      "9471/9471 [==============================] - 25s - loss: 7.2680 - acc: 0.1837    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246f7c778d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "model.fit(X_train,Y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test,Y_test = generate_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re  = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(re)):\n",
    "#     print(vocab_list[sample(re[i])],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data():\n",
    "    with open('tip.json') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            tip = json.loads(line.strip())['text']\n",
    "            vec = one_hot_(i,transnet_size)\n",
    "            X = []\n",
    "            one_hot_v = one_hot('start')\n",
    "            X = [one_hot_v+vec for _ in range(sequence_length)]\n",
    "                \n",
    "            return vec,tip,numpy.array(X).reshape(1,sequence_length,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec,tip,X = generate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the kidding. has even this up be people to easy "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = model.predict(X)\n",
    "#     print(y.shape)\n",
    "#     print (y)\n",
    "    w = sample(y[0])\n",
    "#     print(w)\n",
    "    print(vocab_list[w],end=\" \")\n",
    "    X = numpy.delete(X,0,1)\n",
    "#     print(X.shape)\n",
    "    X = numpy.insert(X,sequence_length-1,one_hot_(w,len(vocab_list))+vec,axis=1)\n",
    "#     print(X.shape)\n",
    "#     print(X[0][sequence_length-1])\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get here early enough to have dinner.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
