{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "filename = 'glove.50000.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = set()\n",
    "    embd = {}\n",
    "    word = None\n",
    "    with open(filename) as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                row = line.strip().split(' ')\n",
    "                word = row[0]\n",
    "                vocab.add(row[0])\n",
    "                embd[word] = np.array([float(x) for x in row[1:]])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(word)\n",
    "    print('Loaded GloVe!')\n",
    "    return vocab,embd\n",
    "vocab_set,embd = loadGloVe(filename)\n",
    "# vocab_size = len(vocab)\n",
    "# vocab_set = set(vocab)\n",
    "# embedding_dim = len(embd[0])\n",
    "# embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_set.add('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd['<UNK>'] = np.zeros(50, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = list(vocab_set)\n",
    "word_to_index = {vocab_list[i]: i for i in range(len(vocab_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('<UNK>' in vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import numpy\n",
    "# def generate_vocabulary():\n",
    "#     i = 0\n",
    "#     vocab = set() \n",
    "#     with open(\"tip.json\",encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             tip = json.loads(line.strip())['text']\n",
    "# #             print (tip)\n",
    "#             words = tip.lower().split()\n",
    "#             vocab |= set(words)\n",
    "#             i +=1\n",
    "#             if i == 5000:\n",
    "#                 break\n",
    "#     vocab.add('start')\n",
    "    \n",
    "#     return vocab,list(vocab)\n",
    "# # def generate_data():\n",
    "# vocab_set,vocab_list = generate_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_index = {vocab_list[i]:i for i in range(len(vocab_list))}\n",
    "transnet_size = 64\n",
    "sequence_length = 3\n",
    "def one_hot(word,word_index,vocab_len):\n",
    "    x = [0]*vocab_len\n",
    "    if word in word_index:\n",
    "        x[word_index[word]] = 1\n",
    "    return x\n",
    "# def one_hot_(k,l):\n",
    "#     x = [0]*(l)\n",
    "#     while k > 0:        \n",
    "#         x[k%l] = 1\n",
    "#         k = k//l\n",
    "#     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def generate_train():\n",
    "#     j = 0\n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     total = 1000\n",
    "#     tnet_file = open('../src/ex2_data/train_epochs/newfile.txt')\n",
    "#     with open(\"./data/train_lex_data.txt\") as f:\n",
    "#         for line in f:\n",
    "#             user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "#             #vec = one_hot_(j,transnet_size)\n",
    "#             words = ['start' for _ in range(sequence_length)] + tip.strip().lower().split()\n",
    "#             vectors = []\n",
    "#             onehot = []\n",
    "#             for word in words:\n",
    "#                 encoding = one_hot(word)\n",
    "#                 onehot += [encoding]\n",
    "#                 vectors += [encoding+vec]\n",
    "#             for i in range(sequence_length-1,len(words)-1):\n",
    "#                 X += [vectors[i-sequence_length+1:i+1]]\n",
    "#                 Y += [onehot[i+1]]\n",
    "#             j += 1\n",
    "#             if j % 100 == 0:\n",
    "#                 print(j)\n",
    "#             if j == total:\n",
    "#                 break\n",
    "#     return np.array(X),np.array(Y)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X,Y =generate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punc = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_item_index = pickle.load(open('user_item_index_dict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_item_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def new_generate_train():\n",
    "    sequence_length = 3\n",
    "    j = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    total = 500\n",
    "    tnet_file = pickle.load(open('./gen_review_vec_train.pkl','rb'))\n",
    "    with open(\"./data/train.txt\") as f:\n",
    "        for line in f:\n",
    "#             rev_emb = np.zeros(50)\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            rev_index = user_item_index[(user_id,item_id)]\n",
    "            \n",
    "            #vec = one_hot_(j,transnet_size)\n",
    "            rev_words = [w.lower() for w in tip.split()]\n",
    "            words = []\n",
    "            for w in rev_words:\n",
    "                prev = 0\n",
    "                for ch in range(len(w)):\n",
    "                    if w[ch] in punc:\n",
    "                        if prev < ch:\n",
    "                            words += [w[prev:ch]]\n",
    "                        words += [w[ch]]\n",
    "                        prev = ch + 1\n",
    "                if prev < len(w):\n",
    "                    words += [w[prev:]]\n",
    "            if len(words)< 5:\n",
    "                words += ['<UNK>']*(5-len(words))\n",
    "            elif len(words) > 5:\n",
    "                words = words[:5]\n",
    "            words = ['<UNK>' for _ in range(sequence_length)] + words\n",
    "            vectors = []\n",
    "            onehot = []\n",
    "            vec = tnet_file[rev_index] #index of review from tnet file\n",
    "            j += 1\n",
    "#             if j % 10 == 0:\n",
    "#                 print(j,end=\" \")\n",
    "            for word in words:\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                onehot += [onehot_encoding]\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "            X = []\n",
    "            Y = []\n",
    "            for i in range(sequence_length-1,len(words)-1):\n",
    "                X += [vectors[i-sequence_length+1:i+1]]\n",
    "                Y += [onehot[i+1]]\n",
    "#             X,Y = np.array(X),np.array(Y)\n",
    "#             print(X.shape,Y.shape)\n",
    "            yield X,Y\n",
    "#                 Y += []\n",
    "            \n",
    "#             if j == total:\n",
    "#                 break\n",
    "#     return X,Y#np.array(X),np.array(Y)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X =[]\n",
    "Y = []\n",
    "for x,y in new_generate_train():\n",
    "    X += x\n",
    "    Y += y\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 3, 114)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 50001)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for x,y in new_generate_train():\n",
    "#     if i % 10 == 0:\n",
    "#         print(i,end=\" \")\n",
    "#     i += 1\n",
    "#     if i == 600:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 3\n",
    "transnet_size = 64\n",
    "word_emb_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(vocab_list)\n",
    "model = Sequential()\n",
    "model.add(LSTM(15,input_shape=(sequence_length,word_emb_size+transnet_size)))\n",
    "model.add(Dense(V,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, Y_train = a,b #generate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 7.8894 - acc: 0.0284\n",
      "Epoch 2/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 6.1491 - acc: 0.0360\n",
      "Epoch 3/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 5.8067 - acc: 0.0544\n",
      "Epoch 4/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 5.5893 - acc: 0.0648\n",
      "Epoch 5/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 5.3899 - acc: 0.0788\n",
      "Epoch 6/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 5.2143 - acc: 0.0884\n",
      "Epoch 7/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 5.0589 - acc: 0.1056\n",
      "Epoch 8/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.9320 - acc: 0.1080\n",
      "Epoch 9/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.8352 - acc: 0.1180\n",
      "Epoch 10/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.7740 - acc: 0.1212\n",
      "Epoch 11/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.7021 - acc: 0.1304\n",
      "Epoch 12/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.6494 - acc: 0.1444\n",
      "Epoch 13/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.5995 - acc: 0.1460\n",
      "Epoch 14/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.5612 - acc: 0.1496\n",
      "Epoch 15/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.5274 - acc: 0.1596\n",
      "Epoch 16/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.5076 - acc: 0.1628\n",
      "Epoch 17/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.4859 - acc: 0.1672\n",
      "Epoch 18/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.4598 - acc: 0.1644\n",
      "Epoch 19/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.4452 - acc: 0.1728\n",
      "Epoch 20/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.4328 - acc: 0.1796\n",
      "Epoch 21/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.4192 - acc: 0.1804\n",
      "Epoch 22/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.4011 - acc: 0.1908\n",
      "Epoch 23/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3851 - acc: 0.1900\n",
      "Epoch 24/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3782 - acc: 0.1896\n",
      "Epoch 25/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3640 - acc: 0.1964\n",
      "Epoch 26/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3500 - acc: 0.1988\n",
      "Epoch 27/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3384 - acc: 0.1972\n",
      "Epoch 28/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3187 - acc: 0.2088\n",
      "Epoch 29/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.3017 - acc: 0.2096\n",
      "Epoch 30/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2854 - acc: 0.2124\n",
      "Epoch 31/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2695 - acc: 0.2152\n",
      "Epoch 32/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2465 - acc: 0.2068\n",
      "Epoch 33/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2435 - acc: 0.2168\n",
      "Epoch 34/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2272 - acc: 0.2164\n",
      "Epoch 35/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2149 - acc: 0.2256\n",
      "Epoch 36/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.2028 - acc: 0.2176\n",
      "Epoch 37/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1724 - acc: 0.2228\n",
      "Epoch 38/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1673 - acc: 0.2244\n",
      "Epoch 39/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1536 - acc: 0.2296\n",
      "Epoch 40/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1458 - acc: 0.2308\n",
      "Epoch 41/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1341 - acc: 0.2308\n",
      "Epoch 42/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1202 - acc: 0.2284\n",
      "Epoch 43/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1098 - acc: 0.2352\n",
      "Epoch 44/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.1047 - acc: 0.2296\n",
      "Epoch 45/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0787 - acc: 0.2376\n",
      "Epoch 46/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0788 - acc: 0.2404\n",
      "Epoch 47/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0698 - acc: 0.2376\n",
      "Epoch 48/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0574 - acc: 0.2412\n",
      "Epoch 49/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0465 - acc: 0.2400\n",
      "Epoch 50/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0305 - acc: 0.2508\n",
      "Epoch 51/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0248 - acc: 0.2472\n",
      "Epoch 52/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0103 - acc: 0.2532\n",
      "Epoch 53/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4.0011 - acc: 0.2512\n",
      "Epoch 54/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9902 - acc: 0.2468\n",
      "Epoch 55/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9914 - acc: 0.2452\n",
      "Epoch 56/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9773 - acc: 0.2528\n",
      "Epoch 57/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9684 - acc: 0.2508\n",
      "Epoch 58/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9556 - acc: 0.2544\n",
      "Epoch 59/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9482 - acc: 0.2560\n",
      "Epoch 60/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9491 - acc: 0.2584\n",
      "Epoch 61/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9384 - acc: 0.2548\n",
      "Epoch 62/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9278 - acc: 0.2612\n",
      "Epoch 63/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9197 - acc: 0.2580\n",
      "Epoch 64/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9147 - acc: 0.2544\n",
      "Epoch 65/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9046 - acc: 0.2596\n",
      "Epoch 66/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.9033 - acc: 0.2564\n",
      "Epoch 67/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8823 - acc: 0.2604\n",
      "Epoch 68/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8850 - acc: 0.2724\n",
      "Epoch 69/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8761 - acc: 0.2688\n",
      "Epoch 70/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8729 - acc: 0.2664\n",
      "Epoch 71/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8605 - acc: 0.2756\n",
      "Epoch 72/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8496 - acc: 0.2716\n",
      "Epoch 73/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8397 - acc: 0.2752\n",
      "Epoch 74/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8273 - acc: 0.2736\n",
      "Epoch 75/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8299 - acc: 0.2728\n",
      "Epoch 76/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8188 - acc: 0.2768\n",
      "Epoch 77/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8188 - acc: 0.2828\n",
      "Epoch 78/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8243 - acc: 0.2804\n",
      "Epoch 79/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8062 - acc: 0.2828\n",
      "Epoch 80/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8115 - acc: 0.2824\n",
      "Epoch 81/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.7998 - acc: 0.2852\n",
      "Epoch 82/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.8014 - acc: 0.2816\n",
      "Epoch 83/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7934 - acc: 0.2860\n",
      "Epoch 84/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7813 - acc: 0.2860\n",
      "Epoch 85/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7869 - acc: 0.2920\n",
      "Epoch 86/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7703 - acc: 0.2880\n",
      "Epoch 87/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7652 - acc: 0.2908\n",
      "Epoch 88/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7595 - acc: 0.2960\n",
      "Epoch 89/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7625 - acc: 0.2960\n",
      "Epoch 90/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7521 - acc: 0.2960\n",
      "Epoch 91/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7470 - acc: 0.2960\n",
      "Epoch 92/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7499 - acc: 0.2936\n",
      "Epoch 93/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7496 - acc: 0.2908\n",
      "Epoch 94/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7385 - acc: 0.2988\n",
      "Epoch 95/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7331 - acc: 0.2936\n",
      "Epoch 96/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7295 - acc: 0.2996\n",
      "Epoch 97/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7222 - acc: 0.2996\n",
      "Epoch 98/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7246 - acc: 0.2948\n",
      "Epoch 99/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7153 - acc: 0.3044\n",
      "Epoch 100/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7139 - acc: 0.3100\n",
      "Epoch 101/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7168 - acc: 0.3044\n",
      "Epoch 102/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7222 - acc: 0.3036\n",
      "Epoch 103/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7141 - acc: 0.3036\n",
      "Epoch 104/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.7065 - acc: 0.3092\n",
      "Epoch 105/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7129 - acc: 0.3076\n",
      "Epoch 106/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.7078 - acc: 0.3052\n",
      "Epoch 107/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7052 - acc: 0.3064\n",
      "Epoch 108/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.7048 - acc: 0.3080\n",
      "Epoch 109/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.7035 - acc: 0.3084\n",
      "Epoch 110/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6994 - acc: 0.3056\n",
      "Epoch 111/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6980 - acc: 0.3092\n",
      "Epoch 112/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6932 - acc: 0.3060\n",
      "Epoch 113/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6868 - acc: 0.3104\n",
      "Epoch 114/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6866 - acc: 0.3072\n",
      "Epoch 115/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6849 - acc: 0.3016\n",
      "Epoch 116/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6805 - acc: 0.3096\n",
      "Epoch 117/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.6765 - acc: 0.3152\n",
      "Epoch 118/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6733 - acc: 0.3112\n",
      "Epoch 119/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6694 - acc: 0.3124\n",
      "Epoch 120/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6668 - acc: 0.3128\n",
      "Epoch 121/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6662 - acc: 0.3116\n",
      "Epoch 122/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6665 - acc: 0.3136\n",
      "Epoch 123/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6797 - acc: 0.3156\n",
      "Epoch 124/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6704 - acc: 0.3160\n",
      "Epoch 125/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6679 - acc: 0.3124\n",
      "Epoch 126/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6584 - acc: 0.3200\n",
      "Epoch 127/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6571 - acc: 0.3156\n",
      "Epoch 128/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6584 - acc: 0.3208\n",
      "Epoch 129/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6553 - acc: 0.3248\n",
      "Epoch 130/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6503 - acc: 0.3176\n",
      "Epoch 131/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6479 - acc: 0.3168\n",
      "Epoch 132/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6467 - acc: 0.3220\n",
      "Epoch 133/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6438 - acc: 0.3092\n",
      "Epoch 134/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6442 - acc: 0.3232\n",
      "Epoch 135/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6438 - acc: 0.3176\n",
      "Epoch 136/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6384 - acc: 0.3200\n",
      "Epoch 137/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6297 - acc: 0.3208\n",
      "Epoch 138/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6337 - acc: 0.3244\n",
      "Epoch 139/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6359 - acc: 0.3284\n",
      "Epoch 140/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6317 - acc: 0.3236\n",
      "Epoch 141/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6337 - acc: 0.3232\n",
      "Epoch 142/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6245 - acc: 0.3220\n",
      "Epoch 143/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6255 - acc: 0.3228\n",
      "Epoch 144/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6234 - acc: 0.3232\n",
      "Epoch 145/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6210 - acc: 0.3248\n",
      "Epoch 146/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6221 - acc: 0.3288\n",
      "Epoch 147/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6184 - acc: 0.3256\n",
      "Epoch 148/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6248 - acc: 0.3252\n",
      "Epoch 149/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6300 - acc: 0.3220\n",
      "Epoch 150/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6186 - acc: 0.3240\n",
      "Epoch 151/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6142 - acc: 0.3300\n",
      "Epoch 152/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6149 - acc: 0.3260\n",
      "Epoch 153/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6106 - acc: 0.3288\n",
      "Epoch 154/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6001 - acc: 0.3240\n",
      "Epoch 155/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6024 - acc: 0.3256\n",
      "Epoch 156/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6102 - acc: 0.3264\n",
      "Epoch 157/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6018 - acc: 0.3280\n",
      "Epoch 158/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6075 - acc: 0.3312\n",
      "Epoch 159/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6068 - acc: 0.3316\n",
      "Epoch 160/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6086 - acc: 0.3260\n",
      "Epoch 161/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5947 - acc: 0.3252\n",
      "Epoch 162/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6036 - acc: 0.3328\n",
      "Epoch 163/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6046 - acc: 0.3296\n",
      "Epoch 164/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6050 - acc: 0.3288\n",
      "Epoch 165/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5944 - acc: 0.3256\n",
      "Epoch 166/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6004 - acc: 0.3308\n",
      "Epoch 167/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5964 - acc: 0.3236\n",
      "Epoch 168/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5963 - acc: 0.3236\n",
      "Epoch 169/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.6010 - acc: 0.3252\n",
      "Epoch 170/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5948 - acc: 0.3232\n",
      "Epoch 171/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5979 - acc: 0.3284\n",
      "Epoch 172/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5918 - acc: 0.3264\n",
      "Epoch 173/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5910 - acc: 0.3316\n",
      "Epoch 174/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5789 - acc: 0.3288\n",
      "Epoch 175/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5806 - acc: 0.3324\n",
      "Epoch 176/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5845 - acc: 0.3272\n",
      "Epoch 177/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5873 - acc: 0.3344\n",
      "Epoch 178/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5850 - acc: 0.3384\n",
      "Epoch 179/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5842 - acc: 0.3240\n",
      "Epoch 180/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5830 - acc: 0.3364\n",
      "Epoch 181/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5872 - acc: 0.3340\n",
      "Epoch 182/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5817 - acc: 0.3328\n",
      "Epoch 183/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5791 - acc: 0.3304\n",
      "Epoch 184/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5829 - acc: 0.3304\n",
      "Epoch 185/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5852 - acc: 0.3308\n",
      "Epoch 186/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5804 - acc: 0.3304\n",
      "Epoch 187/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5699 - acc: 0.3392\n",
      "Epoch 188/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5846 - acc: 0.3332\n",
      "Epoch 189/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5751 - acc: 0.3328\n",
      "Epoch 190/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5827 - acc: 0.3356\n",
      "Epoch 191/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5732 - acc: 0.3300\n",
      "Epoch 192/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5583 - acc: 0.3384\n",
      "Epoch 193/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5650 - acc: 0.3356\n",
      "Epoch 194/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5660 - acc: 0.3360\n",
      "Epoch 195/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5656 - acc: 0.3360\n",
      "Epoch 196/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5692 - acc: 0.3372\n",
      "Epoch 197/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5617 - acc: 0.3348\n",
      "Epoch 198/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5632 - acc: 0.3380\n",
      "Epoch 199/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5653 - acc: 0.3376\n",
      "Epoch 200/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5652 - acc: 0.3368\n",
      "Epoch 201/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5630 - acc: 0.3372\n",
      "Epoch 202/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5612 - acc: 0.3364\n",
      "Epoch 203/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5635 - acc: 0.3364\n",
      "Epoch 204/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5566 - acc: 0.3376\n",
      "Epoch 205/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5553 - acc: 0.3364\n",
      "Epoch 206/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5581 - acc: 0.3408\n",
      "Epoch 207/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5492 - acc: 0.3352\n",
      "Epoch 208/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5528 - acc: 0.3424\n",
      "Epoch 209/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5445 - acc: 0.3420\n",
      "Epoch 210/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5438 - acc: 0.3420\n",
      "Epoch 211/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5516 - acc: 0.3444\n",
      "Epoch 212/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5523 - acc: 0.3336\n",
      "Epoch 213/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5559 - acc: 0.3424\n",
      "Epoch 214/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5496 - acc: 0.3372\n",
      "Epoch 215/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5470 - acc: 0.3432\n",
      "Epoch 216/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5439 - acc: 0.3436\n",
      "Epoch 217/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5479 - acc: 0.3436\n",
      "Epoch 218/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5444 - acc: 0.3380\n",
      "Epoch 219/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5398 - acc: 0.3360\n",
      "Epoch 220/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5410 - acc: 0.3408\n",
      "Epoch 221/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5426 - acc: 0.3436\n",
      "Epoch 222/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5457 - acc: 0.3372\n",
      "Epoch 223/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5445 - acc: 0.3408\n",
      "Epoch 224/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5409 - acc: 0.3432\n",
      "Epoch 225/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5352 - acc: 0.3412\n",
      "Epoch 226/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5383 - acc: 0.3432\n",
      "Epoch 227/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5343 - acc: 0.3420\n",
      "Epoch 228/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5374 - acc: 0.3428\n",
      "Epoch 229/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5344 - acc: 0.3404\n",
      "Epoch 230/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5332 - acc: 0.3432\n",
      "Epoch 231/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5346 - acc: 0.3444\n",
      "Epoch 232/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5359 - acc: 0.3412\n",
      "Epoch 233/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5305 - acc: 0.3492\n",
      "Epoch 234/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5349 - acc: 0.3428\n",
      "Epoch 235/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5376 - acc: 0.3432\n",
      "Epoch 236/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5345 - acc: 0.3388\n",
      "Epoch 237/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5347 - acc: 0.3428\n",
      "Epoch 238/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5272 - acc: 0.3432\n",
      "Epoch 239/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5234 - acc: 0.3392\n",
      "Epoch 240/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5299 - acc: 0.3468\n",
      "Epoch 241/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5253 - acc: 0.3460\n",
      "Epoch 242/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5177 - acc: 0.3468\n",
      "Epoch 243/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5170 - acc: 0.3440\n",
      "Epoch 244/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5149 - acc: 0.3460\n",
      "Epoch 245/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5142 - acc: 0.3452\n",
      "Epoch 246/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5090 - acc: 0.3484\n",
      "Epoch 247/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5088 - acc: 0.3496\n",
      "Epoch 248/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5060 - acc: 0.3508\n",
      "Epoch 249/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5098 - acc: 0.3396\n",
      "Epoch 250/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5117 - acc: 0.3476\n",
      "Epoch 251/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5078 - acc: 0.3436\n",
      "Epoch 252/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5070 - acc: 0.3452\n",
      "Epoch 253/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4949 - acc: 0.3488\n",
      "Epoch 254/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5141 - acc: 0.3452\n",
      "Epoch 255/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5079 - acc: 0.3488\n",
      "Epoch 256/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5062 - acc: 0.3464\n",
      "Epoch 257/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5097 - acc: 0.3480\n",
      "Epoch 258/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5037 - acc: 0.3536\n",
      "Epoch 259/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4944 - acc: 0.3520\n",
      "Epoch 260/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5110 - acc: 0.3464\n",
      "Epoch 261/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5087 - acc: 0.3488\n",
      "Epoch 262/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5143 - acc: 0.3500\n",
      "Epoch 263/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5057 - acc: 0.3480\n",
      "Epoch 264/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5063 - acc: 0.3428\n",
      "Epoch 265/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5043 - acc: 0.3456\n",
      "Epoch 266/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5067 - acc: 0.3520\n",
      "Epoch 267/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5017 - acc: 0.3480\n",
      "Epoch 268/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5084 - acc: 0.3476\n",
      "Epoch 269/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5022 - acc: 0.3472\n",
      "Epoch 270/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5048 - acc: 0.3476\n",
      "Epoch 271/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5123 - acc: 0.3424\n",
      "Epoch 272/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5088 - acc: 0.3484\n",
      "Epoch 273/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5036 - acc: 0.3496\n",
      "Epoch 274/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5076 - acc: 0.3484\n",
      "Epoch 275/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5019 - acc: 0.3488\n",
      "Epoch 276/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5069 - acc: 0.3440\n",
      "Epoch 277/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4970 - acc: 0.3484\n",
      "Epoch 278/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5056 - acc: 0.3452\n",
      "Epoch 279/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5038 - acc: 0.3452\n",
      "Epoch 280/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4996 - acc: 0.3468\n",
      "Epoch 281/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5010 - acc: 0.3480\n",
      "Epoch 282/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.5025 - acc: 0.3492\n",
      "Epoch 283/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4890 - acc: 0.3540\n",
      "Epoch 284/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4911 - acc: 0.3512\n",
      "Epoch 285/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4888 - acc: 0.3480\n",
      "Epoch 286/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4914 - acc: 0.3556\n",
      "Epoch 287/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4918 - acc: 0.3476\n",
      "Epoch 288/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4960 - acc: 0.3524\n",
      "Epoch 289/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4913 - acc: 0.3492\n",
      "Epoch 290/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4993 - acc: 0.3504\n",
      "Epoch 291/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4945 - acc: 0.3492\n",
      "Epoch 292/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4945 - acc: 0.3524\n",
      "Epoch 293/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4887 - acc: 0.3492\n",
      "Epoch 294/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4899 - acc: 0.3488\n",
      "Epoch 295/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4929 - acc: 0.3536\n",
      "Epoch 296/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4947 - acc: 0.3508\n",
      "Epoch 297/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4897 - acc: 0.3512\n",
      "Epoch 298/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4935 - acc: 0.3560\n",
      "Epoch 299/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4863 - acc: 0.3512\n",
      "Epoch 300/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4854 - acc: 0.3472\n",
      "Epoch 301/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4950 - acc: 0.3500\n",
      "Epoch 302/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4897 - acc: 0.3540\n",
      "Epoch 303/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4881 - acc: 0.3472\n",
      "Epoch 304/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4950 - acc: 0.3520\n",
      "Epoch 305/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4792 - acc: 0.3528\n",
      "Epoch 306/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4782 - acc: 0.3520\n",
      "Epoch 307/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4844 - acc: 0.3504\n",
      "Epoch 308/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4725 - acc: 0.3436\n",
      "Epoch 309/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4841 - acc: 0.3544\n",
      "Epoch 310/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4746 - acc: 0.3512\n",
      "Epoch 311/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4844 - acc: 0.3508\n",
      "Epoch 312/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4888 - acc: 0.3536\n",
      "Epoch 313/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4763 - acc: 0.3484\n",
      "Epoch 314/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4785 - acc: 0.3532\n",
      "Epoch 315/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4766 - acc: 0.3476\n",
      "Epoch 316/500\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 3.4724 - acc: 0.3556\n",
      "Epoch 317/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4710 - acc: 0.3500\n",
      "Epoch 318/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4667 - acc: 0.3504\n",
      "Epoch 319/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4710 - acc: 0.3556\n",
      "Epoch 320/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4688 - acc: 0.3516\n",
      "Epoch 321/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4669 - acc: 0.3496\n",
      "Epoch 322/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4722 - acc: 0.3480\n",
      "Epoch 323/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4659 - acc: 0.3488\n",
      "Epoch 324/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4731 - acc: 0.3556\n",
      "Epoch 325/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4660 - acc: 0.3516\n",
      "Epoch 326/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4700 - acc: 0.3592\n",
      "Epoch 327/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4661 - acc: 0.3580\n",
      "Epoch 328/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4696 - acc: 0.3480\n",
      "Epoch 329/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4608 - acc: 0.3520\n",
      "Epoch 330/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4651 - acc: 0.3504\n",
      "Epoch 331/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4623 - acc: 0.3516\n",
      "Epoch 332/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4589 - acc: 0.3568\n",
      "Epoch 333/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4624 - acc: 0.3552\n",
      "Epoch 334/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4677 - acc: 0.3516\n",
      "Epoch 335/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4579 - acc: 0.3564\n",
      "Epoch 336/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4608 - acc: 0.3480\n",
      "Epoch 337/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4659 - acc: 0.3516\n",
      "Epoch 338/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4626 - acc: 0.3532\n",
      "Epoch 339/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4657 - acc: 0.3572\n",
      "Epoch 340/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4637 - acc: 0.3536\n",
      "Epoch 341/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4646 - acc: 0.3564\n",
      "Epoch 342/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4585 - acc: 0.3552\n",
      "Epoch 343/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4524 - acc: 0.3568\n",
      "Epoch 344/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4588 - acc: 0.3520\n",
      "Epoch 345/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4576 - acc: 0.3556\n",
      "Epoch 346/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4533 - acc: 0.3528\n",
      "Epoch 347/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4521 - acc: 0.3536\n",
      "Epoch 348/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4553 - acc: 0.3612\n",
      "Epoch 349/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4480 - acc: 0.3572\n",
      "Epoch 350/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4615 - acc: 0.3584\n",
      "Epoch 351/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4590 - acc: 0.3548\n",
      "Epoch 352/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4595 - acc: 0.3500\n",
      "Epoch 353/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4596 - acc: 0.3628\n",
      "Epoch 354/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4676 - acc: 0.3576\n",
      "Epoch 355/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4548 - acc: 0.3576\n",
      "Epoch 356/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4518 - acc: 0.3540\n",
      "Epoch 357/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4539 - acc: 0.3592\n",
      "Epoch 358/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4541 - acc: 0.3580\n",
      "Epoch 359/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4506 - acc: 0.3576\n",
      "Epoch 360/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4457 - acc: 0.3600\n",
      "Epoch 361/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4448 - acc: 0.3608\n",
      "Epoch 362/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4426 - acc: 0.3556\n",
      "Epoch 363/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4372 - acc: 0.3584\n",
      "Epoch 364/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4414 - acc: 0.3608\n",
      "Epoch 365/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4419 - acc: 0.3588\n",
      "Epoch 366/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4418 - acc: 0.3592\n",
      "Epoch 367/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4434 - acc: 0.3596\n",
      "Epoch 368/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4517 - acc: 0.3592\n",
      "Epoch 369/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4441 - acc: 0.3576\n",
      "Epoch 370/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4491 - acc: 0.3552\n",
      "Epoch 371/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4437 - acc: 0.3544\n",
      "Epoch 372/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4484 - acc: 0.3560\n",
      "Epoch 373/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4471 - acc: 0.3600\n",
      "Epoch 374/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4471 - acc: 0.3600\n",
      "Epoch 375/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4469 - acc: 0.3640\n",
      "Epoch 376/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4448 - acc: 0.3576\n",
      "Epoch 377/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4466 - acc: 0.3504\n",
      "Epoch 378/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4448 - acc: 0.3616\n",
      "Epoch 379/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4441 - acc: 0.3604\n",
      "Epoch 380/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4505 - acc: 0.3604\n",
      "Epoch 381/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4390 - acc: 0.3568\n",
      "Epoch 382/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4465 - acc: 0.3588\n",
      "Epoch 383/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4450 - acc: 0.3568\n",
      "Epoch 384/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4429 - acc: 0.3536\n",
      "Epoch 385/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4392 - acc: 0.3604\n",
      "Epoch 386/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4364 - acc: 0.3620\n",
      "Epoch 387/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4441 - acc: 0.3592\n",
      "Epoch 388/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4486 - acc: 0.3548\n",
      "Epoch 389/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4436 - acc: 0.3596\n",
      "Epoch 390/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4533 - acc: 0.3560\n",
      "Epoch 391/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4402 - acc: 0.3660\n",
      "Epoch 392/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4513 - acc: 0.3588\n",
      "Epoch 393/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4455 - acc: 0.3576\n",
      "Epoch 394/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4348 - acc: 0.3612\n",
      "Epoch 395/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4400 - acc: 0.3624\n",
      "Epoch 396/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4410 - acc: 0.3596\n",
      "Epoch 397/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4448 - acc: 0.3604\n",
      "Epoch 398/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4311 - acc: 0.3624\n",
      "Epoch 399/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4383 - acc: 0.3624\n",
      "Epoch 400/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4354 - acc: 0.3660\n",
      "Epoch 401/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4267 - acc: 0.3600\n",
      "Epoch 402/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4286 - acc: 0.3564\n",
      "Epoch 403/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4303 - acc: 0.3636\n",
      "Epoch 404/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4268 - acc: 0.3632\n",
      "Epoch 405/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4287 - acc: 0.3620\n",
      "Epoch 406/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4394 - acc: 0.3600\n",
      "Epoch 407/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4300 - acc: 0.3604\n",
      "Epoch 408/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4188 - acc: 0.3588\n",
      "Epoch 409/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4359 - acc: 0.3600\n",
      "Epoch 410/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4265 - acc: 0.3600\n",
      "Epoch 411/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4231 - acc: 0.3592\n",
      "Epoch 412/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4295 - acc: 0.3664\n",
      "Epoch 413/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4265 - acc: 0.3608\n",
      "Epoch 414/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4262 - acc: 0.3644\n",
      "Epoch 415/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4271 - acc: 0.3572\n",
      "Epoch 416/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4174 - acc: 0.3652\n",
      "Epoch 417/500\n",
      "2500/2500 [==============================] - 11s 5ms/step - loss: 3.4316 - acc: 0.3604\n",
      "Epoch 418/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4225 - acc: 0.3588\n",
      "Epoch 419/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4249 - acc: 0.3636\n",
      "Epoch 420/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4273 - acc: 0.3576\n",
      "Epoch 421/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4255 - acc: 0.3612\n",
      "Epoch 422/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4190 - acc: 0.3608\n",
      "Epoch 423/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4234 - acc: 0.3600\n",
      "Epoch 424/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4243 - acc: 0.3640\n",
      "Epoch 425/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4185 - acc: 0.3664\n",
      "Epoch 426/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4266 - acc: 0.3624\n",
      "Epoch 427/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4176 - acc: 0.3604\n",
      "Epoch 428/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4195 - acc: 0.3616\n",
      "Epoch 429/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4215 - acc: 0.3624\n",
      "Epoch 430/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4181 - acc: 0.3612\n",
      "Epoch 431/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4194 - acc: 0.3596\n",
      "Epoch 432/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4216 - acc: 0.3664\n",
      "Epoch 433/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4146 - acc: 0.3596\n",
      "Epoch 434/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4296 - acc: 0.3576\n",
      "Epoch 435/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4294 - acc: 0.3568\n",
      "Epoch 436/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4203 - acc: 0.3640\n",
      "Epoch 437/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4244 - acc: 0.3592\n",
      "Epoch 438/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4257 - acc: 0.3604\n",
      "Epoch 439/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4246 - acc: 0.3596\n",
      "Epoch 440/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4233 - acc: 0.3632\n",
      "Epoch 441/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4191 - acc: 0.3632\n",
      "Epoch 442/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4148 - acc: 0.3616\n",
      "Epoch 443/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4167 - acc: 0.3580\n",
      "Epoch 444/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4147 - acc: 0.3604\n",
      "Epoch 445/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4181 - acc: 0.3612\n",
      "Epoch 446/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4062 - acc: 0.3648\n",
      "Epoch 447/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4087 - acc: 0.3640\n",
      "Epoch 448/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4111 - acc: 0.3628\n",
      "Epoch 449/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4032 - acc: 0.3708\n",
      "Epoch 450/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4170 - acc: 0.3600\n",
      "Epoch 451/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4154 - acc: 0.3672\n",
      "Epoch 452/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4125 - acc: 0.3656\n",
      "Epoch 453/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4152 - acc: 0.3672\n",
      "Epoch 454/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4174 - acc: 0.3580\n",
      "Epoch 455/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4194 - acc: 0.3604\n",
      "Epoch 456/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4098 - acc: 0.3624\n",
      "Epoch 457/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4107 - acc: 0.3632\n",
      "Epoch 458/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.3996 - acc: 0.3680\n",
      "Epoch 459/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4063 - acc: 0.3684\n",
      "Epoch 460/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4092 - acc: 0.3640\n",
      "Epoch 461/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4060 - acc: 0.3636\n",
      "Epoch 462/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4033 - acc: 0.3624\n",
      "Epoch 463/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4104 - acc: 0.3604\n",
      "Epoch 464/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4161 - acc: 0.3644\n",
      "Epoch 465/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4027 - acc: 0.3656\n",
      "Epoch 466/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4091 - acc: 0.3664\n",
      "Epoch 467/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4099 - acc: 0.3660\n",
      "Epoch 468/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4191 - acc: 0.3640\n",
      "Epoch 469/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4184 - acc: 0.3616\n",
      "Epoch 470/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4042 - acc: 0.3672\n",
      "Epoch 471/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4121 - acc: 0.3676\n",
      "Epoch 472/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4087 - acc: 0.3672\n",
      "Epoch 473/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4133 - acc: 0.3668\n",
      "Epoch 474/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4123 - acc: 0.3708\n",
      "Epoch 475/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4029 - acc: 0.3692\n",
      "Epoch 476/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.3980 - acc: 0.3624\n",
      "Epoch 477/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4026 - acc: 0.3692\n",
      "Epoch 478/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4070 - acc: 0.3656\n",
      "Epoch 479/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4067 - acc: 0.3648\n",
      "Epoch 480/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.3977 - acc: 0.3696\n",
      "Epoch 481/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4043 - acc: 0.3684\n",
      "Epoch 482/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4038 - acc: 0.3680\n",
      "Epoch 483/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4067 - acc: 0.3664\n",
      "Epoch 484/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4070 - acc: 0.3644\n",
      "Epoch 485/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4003 - acc: 0.3656\n",
      "Epoch 486/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4046 - acc: 0.3684\n",
      "Epoch 487/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.3982 - acc: 0.3676\n",
      "Epoch 488/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.3993 - acc: 0.3684\n",
      "Epoch 489/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4086 - acc: 0.3652\n",
      "Epoch 490/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4004 - acc: 0.3664\n",
      "Epoch 491/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.3985 - acc: 0.3720\n",
      "Epoch 492/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4000 - acc: 0.3708\n",
      "Epoch 493/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.3962 - acc: 0.3668\n",
      "Epoch 494/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4030 - acc: 0.3660\n",
      "Epoch 495/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4017 - acc: 0.3680\n",
      "Epoch 496/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.3959 - acc: 0.3636\n",
      "Epoch 497/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.3970 - acc: 0.3796\n",
      "Epoch 498/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4017 - acc: 0.3684\n",
      "Epoch 499/500\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3.4067 - acc: 0.3672\n",
      "Epoch 500/500\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3.4048 - acc: 0.3692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d587710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "model.fit(X,Y,nb_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#sample\n",
    "import numpy as np\n",
    "def long_to_small(preds):\n",
    "    preds = preds.tolist()\n",
    "    index = []\n",
    "    small_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] > 0.001:\n",
    "            small_preds += [preds[i]]\n",
    "            index += [i]\n",
    "    return small_preds,index\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    #print preds[:10], np.max(preds)\n",
    "    #print(len(preds))\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_test,Y_test = generate_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re  = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(re)):\n",
    "#     print(vocab_list[sample(re[i])],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def test(out):\n",
    "    sequence_length = 3\n",
    "    \n",
    "    j = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    #total = 1000\n",
    "    tnet_file = pickle.load(open('./gen_review_vec_test.pkl','rb')) #changr to test\n",
    "    with open(\"./data/Old/test_lex_data.txt\") as f:\n",
    "        for line in f:\n",
    "            rev_emb = np.zeros(50)\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            #vec = one_hot_(j,transnet_size)\n",
    "#             rev_words = [w.lower() for w in review_text.split()]\n",
    "            words = []\n",
    "            words = ['<UNK>' for _ in range(sequence_length)] + words\n",
    "            vectors = []\n",
    "            onehot = []\n",
    "            vec = tnet_file[j]\n",
    "            for word in words:\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                onehot += [onehot_encoding]\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "            #for i in range(sequence_length-1,len(words)-1):\n",
    "            X += [vectors[0:sequence_length]]\n",
    "#                 Y += [onehot[i+1]]\n",
    "            j += 1\n",
    "            yp = model.predict(np.array(X))[0]\n",
    "            yp_small, index = long_to_small(yp)\n",
    "            yp = vocab_list[index[sample(yp_small)]]\n",
    "            generated_tip = [yp]\n",
    "            words += [yp]\n",
    "            for i in range(sequence_length,10+sequence_length):\n",
    "                word = generated_tip[-1]\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                onehot += [onehot_encoding]\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "                X += [vectors[i-sequence_length+1:i+1]]\n",
    "                x = [X[-1]]\n",
    "                yp = model.predict(np.array(x))[0]\n",
    "                yp_small, index = long_to_small(yp)\n",
    "                yp = vocab_list[index[sample(yp_small)]]\n",
    "                generated_tip += [yp]\n",
    "                words += [yp]\n",
    "            out += [\" \".join(generated_tip)]\n",
    "            if j == 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great this is we <UNK> i . . live locals good',\n",
       " 'super , . . one . . . with essen head',\n",
       " 'location and . get fresh . and they nice ! go',\n",
       " 'nice sushi really need spot ! clean great afraid some i',\n",
       " 'location shannon and awesome delicious if , spot can tea my']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['julie food . good food . die vogel favorite with lots',\n",
       " \"an town this priced the terrible product down ' this not\",\n",
       " 'this \" - selection a amazing time . . my a',\n",
       " 'hoppy great place . fresh and 32 ink see down are',\n",
       " 'a week to - photo hooked slim right a fresh and']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 150 epochs\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def test(out):\n",
    "    sequence_length = 3\n",
    "    \n",
    "    j = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    #total = 1000\n",
    "    tnet_file = pickle.load(open('./gen_review_vec_test.pkl','rb')) #changr to test\n",
    "    with open(\"./data/Old/test_lex_2000.txt\") as f:\n",
    "        for line in f:\n",
    "            rev_emb = np.zeros(50)\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            #vec = one_hot_(j,transnet_size)\n",
    "#             rev_words = [w.lower() for w in review_text.split()]\n",
    "            words = []\n",
    "            words = ['<UNK>' for _ in range(sequence_length)] + words\n",
    "            vectors = []\n",
    "            onehot = []\n",
    "            vec = tnet_file[j]\n",
    "            for word in words:\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                onehot += [onehot_encoding]\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "            #for i in range(sequence_length-1,len(words)-1):\n",
    "            X += [vectors[0:sequence_length]]\n",
    "#                 Y += [onehot[i+1]]\n",
    "            j += 1\n",
    "            yp = model.predict(np.array(X))[0]\n",
    "            yp_small, index = long_to_small(yp)\n",
    "            yp = vocab_list[index[sample(yp_small)]]\n",
    "            generated_tip = [yp]\n",
    "            words += [yp]\n",
    "            for i in range(sequence_length,10+sequence_length):\n",
    "                word = generated_tip[-1]\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                onehot += [onehot_encoding]\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "                X += [vectors[i-sequence_length+1:i+1]]\n",
    "                x = [X[-1]]\n",
    "                yp = model.predict(np.array(x))[0]\n",
    "                yp_small, index = long_to_small(yp)\n",
    "                yp = vocab_list[index[sample(yp_small)]]\n",
    "                generated_tip += [yp]\n",
    "                words += [yp]\n",
    "            out += [\" \".join(generated_tip)]\n",
    "            if j%50 == 0:\n",
    "                print j,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 2000\n"
     ]
    }
   ],
   "source": [
    "test(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "fopen = open('./Result_RNN.txt','w')\n",
    "with open('./data/Old/test_lex_2000.txt') as f:\n",
    "    for line in f:\n",
    "        _,_,_,_,_,tip = line.split(\"\\t\")\n",
    "        fopen.write(out2[i] + \"\\t\" + tip)\n",
    "        i += 1\n",
    "fopen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beef great favorite open . lots to poutine i it .'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_out = open('RNN_out.txt','w')\n",
    "for line in out2:\n",
    "    rnn_out.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_data():\n",
    "    with open('tip.json') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            tip = json.loads(line.strip())['text']\n",
    "            vec = one_hot_(i,transnet_size)\n",
    "            X = []\n",
    "            one_hot_v = one_hot('start')\n",
    "            X = [one_hot_v+vec for _ in range(sequence_length)]\n",
    "                \n",
    "            return vec,tip,numpy.array(X).reshape(1,sequence_length,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec,tip,X = generate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the kidding. has even this up be people to easy "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = model.predict(X)\n",
    "#     print(y.shape)\n",
    "#     print (y)\n",
    "    w = sample(y[0])\n",
    "#     print(w)\n",
    "    print(vocab_list[w],end=\" \")\n",
    "    X = numpy.delete(X,0,1)\n",
    "#     print(X.shape)\n",
    "    X = numpy.insert(X,sequence_length-1,one_hot_(w,len(vocab_list))+vec,axis=1)\n",
    "#     print(X.shape)\n",
    "#     print(X[0][sequence_length-1])\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get here early enough to have dinner.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0677953 ,  0.12908284,  0.00995188, ...,  0.09868402,\n",
       "         0.05035567,  0.0261237 ],\n",
       "       [ 0.0677953 ,  0.12908284,  0.00995188, ...,  0.09868402,\n",
       "         0.05035567,  0.0261237 ],\n",
       "       [ 0.0677953 ,  0.12908284,  0.00995188, ...,  0.09868402,\n",
       "         0.05035567,  0.0261237 ],\n",
       "       ..., \n",
       "       [ 0.06633914,  0.1292228 ,  0.00890549, ...,  0.09720803,\n",
       "         0.04496233,  0.02088072],\n",
       "       [ 0.07077023,  0.13585889,  0.01115206, ...,  0.09501065,\n",
       "         0.03852132,  0.01822961],\n",
       "       [ 0.06911414,  0.13658382,  0.01111554, ...,  0.09772882,\n",
       "         0.04012528,  0.02351698]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('./gen_review_vec_train.pkl','rb'),encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open ('data/train.txt','w',encoding='utf-8')\n",
    "i = 0\n",
    "with open(\"./data/train_lex_data.txt\",encoding='utf-8') as f:\n",
    "        for line    in f:\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            if len(tip) < 25:\n",
    "                continue\n",
    "            file.write(line)\n",
    "            i += 1\n",
    "            if i == 500:\n",
    "                break\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
