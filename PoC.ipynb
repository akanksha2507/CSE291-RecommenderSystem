{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(\"data\",\"yelp_2017\",\"dict.pkl\"),\"rb\") as f:\n",
    "#     word_to_ind = pickle.load(f,encoding=\"bytes\")\n",
    "# with open(os.path.join(\"data\",\"yelp_2017\",\"word_emb.pkl\"),\"rb\") as f:\n",
    "#     emb = pickle.load(f,encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(word_to_ind.values()),emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('yelp_dataset.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "def Select(q):\n",
    "    return c.execute(q)\n",
    "\n",
    "def Update(q):\n",
    "    c.execute(q)\n",
    "    conn.commit()\n",
    "    \n",
    "        \n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=Update(\"create view RevAB_CA as select \\\n",
    "# User.id as user_id, User.review_count as user_review_count, User.average_rating as user_rating, \\\n",
    "# Business.city as city, Business.state as state, Business.ratings as item_rating, Business.id as item_id, \\\n",
    "# Reviews.id as review_id, Reviews.ts as ts, Reviews.rating as review_rating \\\n",
    "# from Reviews \\\n",
    "# Inner JOIN User on User.id = Reviews.user_id \\\n",
    "# INNER JOIN Business on Business.id = Reviews.business_id \\\n",
    "# where Business.city = \\\"Las Vegas\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Select(\"Select * from RevAB_CA where user_review_count > 15 LIMIT 10000\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = data\n",
    "data = []\n",
    "for row in query:\n",
    "    data += [row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(set([x[6] for x in data]))\n",
    "users = list(set([x[0] for x in data]))\n",
    "# items_ind = {items[i]:i for i in range(len(items))}\n",
    "\n",
    "# users_ind = {users[i]:i for i in range(len(users))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015, 5661, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items),len(users),len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "rev_u = defaultdict(lambda: defaultdict(dict))\n",
    "rev_i = defaultdict(lambda: defaultdict(dict))\n",
    "for review in data:\n",
    "    user = review[0]\n",
    "    item = review[6]\n",
    "    text = review[10]#o\n",
    "    rev_u [user][item] = text\n",
    "    rev_i [item][user] = text\n",
    "rev_u = dict(rev_u)\n",
    "rev_i = dict(rev_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "import numpy as np\n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = set()\n",
    "    embd = {}\n",
    "    file = open(filename,'r')\n",
    "    word = None\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                row = line.strip().split(' ')\n",
    "                word = row[0]\n",
    "                vocab.add(row[0])\n",
    "                embd[word] = np.array(row[1:])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(word)\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab_set,embd = loadGloVe(filename)\n",
    "# vocab_size = len(vocab)\n",
    "# vocab_set = set(vocab)\n",
    "# embedding_dim = len(embd[0])\n",
    "# embedding = np.asarray(embd)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers import Dense,Dropout,BatchNormalization, SimpleRNN, GRU, LSTM,Bidirectional,Conv2D,Merge,Flatten,MaxPooling1D#, FactorizationMachinesLayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, activations\n",
    "\n",
    "class FactorizationMachinesLayer(Layer):\n",
    "    '''Factorization Machines layer.\n",
    "\n",
    "    # Arguments\n",
    "        output_dim: int > 0.\n",
    "        k: k of Factorization Machines\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights\n",
    "            initialization. This parameter is only relevant\n",
    "            if you don't pass a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of Numpy arrays to set as initial weights.\n",
    "            The list should have 2 elements, of shape `(input_dim, output_dim)`\n",
    "            and (output_dim,) for weights and biases respectively.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        bias: whether to include a bias\n",
    "            (i.e. make the layer affine rather than linear).\n",
    "        input_dim: dimensionality of the input (integer). This argument\n",
    "            (or alternatively, the keyword argument `input_shape`)\n",
    "            is required when using this layer as the first layer in a model.\n",
    "\n",
    "    # Input shape\n",
    "        nD tensor with shape: `(nb_samples, ..., input_dim)`.\n",
    "        The most common situation would be\n",
    "        a 2D input with shape `(nb_samples, input_dim)`.\n",
    "\n",
    "    # Output shape\n",
    "        nD tensor with shape: `(nb_samples, ..., output_dim)`.\n",
    "        For instance, for a 2D input with shape `(nb_samples, input_dim)`,\n",
    "        the output would have shape `(nb_samples, output_dim)`.\n",
    "    '''\n",
    "    def __init__(self, output_dim, init='glorot_uniform',\n",
    "                 activation=None, weights=None, k=2,\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, input_dim=None, **kwargs):\n",
    "        self.init = initializers.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.k = k\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.initial_weights = weights\n",
    "#         self.input_spec = [InputSpec(ndim='2+')]\n",
    "        \n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(FactorizationMachinesLayer, self).__init__(**kwargs)\n",
    "        print(\"jnsea\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        print(\"build\")\n",
    "        input_dim = input_shape[-1]\n",
    "        self.input_dim = input_dim\n",
    "        print(\"input_dim\",input_dim)\n",
    "#         self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "#                                      ndim='2+')]\n",
    "# \n",
    "        self.W = self.add_weight((input_dim, self.output_dim),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.V = self.add_weight((self.output_dim, input_dim, self.k),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_V'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.output_dim,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        \n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "        print(\"buildN\")\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.sum(K.square(K.dot(x, self.V)) - K.dot(K.square(x), K.square(self.V)), 2)/2\n",
    "        output += K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            output += self.b\n",
    "        return self.activation(output)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        print(self.input_dim)\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1] and input_shape[-1] == self.input_dim\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.output_dim\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(FactorizationMachinesLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "# def FM():\n",
    "# #     model = Sequential()\n",
    "#     inputs = keras.layers.Input(shape=(100,))\n",
    "#     fc_ = Dense(100)(inputs)\n",
    "# #     model.add(Reshape((1,100)))\n",
    "#     fm = FactorizationMachinesLayer(output_dim=1)(fc_)#,k=8,activation=\"relu\", init=\"TruncatedNormal\"))\n",
    "# #     model.add(Flatten())\n",
    "# #     fc = Dense(1)(fc_)\n",
    "#     return keras.models.Model(inputs=inputs,outputs=fm)\n",
    "# # fm = FactorizationMachinesLayer(1,k=8)\n",
    "# # fm.build(input_shape=(1,100))\n",
    "# # fm.get_output_shape_for((5,100))\n",
    "# FM()#.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepCONN(user_shape,item_shape):\n",
    "    model1 = Sequential()\n",
    "    model1.add (Conv2D(32, (3,user_shape[1]),\n",
    "                 input_shape=user_shape, activation=\"tanh\"))\n",
    "       \n",
    "#     model1.add(Flatten())\n",
    "#     model1.add(Reshape((1,)))\n",
    "#     model1.add(MaxPooling1D(pool_size=(32)))  \n",
    "    model1.add(Dense(1,activation=\"relu\"))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(50,activation=\"relu\"))\n",
    "    model1.add(BatchNormalization())\n",
    "                     \n",
    "                     \n",
    "    model2 = Sequential()\n",
    "    model2.add (Conv2D(32, (3,item_shape[1]),\n",
    "                 input_shape=item_shape, activation=\"tanh\"))\n",
    "#     model2.add(Flatten())\n",
    "#     model2.add(Reshape((1,)))\n",
    "#     model2.add(MaxPooling1D(pool_size=(32)))\n",
    "    model2.add(Dense(1,activation=\"relu\"))\n",
    "    model2.add(Flatten())\n",
    "#     model2.Reshape(())\n",
    "    model2.add(Dense(50,activation=\"relu\"))\n",
    "    model2.add(BatchNormalization())\n",
    "                     \n",
    "    model = Sequential()\n",
    "    model.add(Merge([model1,model2],mode='concat'))\n",
    "#     model.add(Dense(32,activation=\"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "#     model1.add(BatchNormalization())\n",
    "#     model.add(Reshape((100,1)))\n",
    "    \n",
    "#     _model = Sequential()\n",
    "#     _model.add(Merge([model,model],mode=\"sum\"))\n",
    "#     model.add(FactorizationMachinesLayer(output_dim=1,k=8,activation=\"relu\", init=\"TruncatedNormal\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    return model,model1,model2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_16 (Merge)             (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,131\n",
      "Trainable params: 14,931\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_48 (Conv2D)           (None, 48, 1, 32)         4832      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 48, 1, 1)          33        \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 50)                2450      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 50)                200       \n",
      "=================================================================\n",
      "Total params: 7,515\n",
      "Trainable params: 7,415\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 48, 1, 32)         4832      \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 48, 1, 1)          33        \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 50)                2450      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 50)                200       \n",
      "=================================================================\n",
      "Total params: 7,515\n",
      "Trainable params: 7,415\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model,model1,model2 = deepCONN((max_len,50,1),(max_len,50,1))\n",
    "model.summary()\n",
    "model1.summary()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy \n",
    "# def embedding(words):\n",
    "#     embed = []\n",
    "#     for word in words:\n",
    "#         embed += [embd[word]]\n",
    "#     for _ in range(100-len(words)):\n",
    "#         embed += [[0]*50]\n",
    "#     return numpy.array(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def embedding(text,maxlen):\n",
    "    text = text.split()\n",
    "    count = 0\n",
    "    _w = []\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word in vocab_set:\n",
    "            _w += [embd[word]]\n",
    "        elif word[:-1] in vocab_set and word[-1] == \".\":\n",
    "            _w += [embd[word[:-1]]]\n",
    "            if len(_w) == max_len:\n",
    "                break\n",
    "            _w += [embd[word[-1]]]\n",
    "        else:\n",
    "            count += 1\n",
    "        if len(_w) == max_len:\n",
    "            break\n",
    "#     print(len(_w))\n",
    "    for _ in range(maxlen-len(_w)):\n",
    "        _w += [numpy.array([0]*50)]\n",
    "    ar= numpy.array(_w)\n",
    "    ar = numpy.reshape(ar,(maxlen,50,1))\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb, item_emb = {},{}\n",
    "for user in rev_u:\n",
    "    user_emb[user] = embedding(\" \".join([rev_u[user][item] for item in rev_u[user] ]),max_len)\n",
    "for item in rev_i:\n",
    "    item_emb[item] = embedding(\" \".join([rev_i[item][user] for user in rev_i[item]]),max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for user in rev_u:\n",
    "#     user_emb[user] = embedding(user_emb[user])\n",
    "# for item in rev_i:\n",
    "#     item_emb[item] = embedding(item_emb[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50, 50, 1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_user = []\n",
    "X_item = []\n",
    "Y = []\n",
    "for i in range(5000):\n",
    "    user,item,rating = data[i][0],data[i][6],data[i][9]\n",
    "#     print(embedding(data[i][10],max_len).shape)\n",
    "    X_user += [user_emb[user]]\n",
    "    X_item += [item_emb[item]]\n",
    "#     print(user_emb[user].shape,item_emb[item].shape)\n",
    "    Y += [rating*1.0/5]\n",
    "X_user = numpy.array(X_user)\n",
    "X_item = numpy.array(X_item)\n",
    "Y = numpy.array(Y)\n",
    "# embd[\"and\"].shape\n",
    "X_user.shape\n",
    "X_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 17s - loss: 0.0855 - acc: 0.3716    \n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0701 - acc: 0.3904    \n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0674 - acc: 0.3906    \n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0667 - acc: 0.3912    \n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0653 - acc: 0.3908    - ETA: 0s - loss: 0.0658 - acc: \n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0653 - acc: 0.3892    \n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0640 - acc: 0.3890    \n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0650 - acc: 0.3880    \n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0637 - acc: 0.3884    \n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 15s - loss: 0.0625 - acc: 0.3886    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb308dbd30>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\",optimizer=keras.optimizers.RMSprop(lr=0.002),metrics=[\"accuracy\"])\n",
    "model.fit([X_user,X_item],Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
