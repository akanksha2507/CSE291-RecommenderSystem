{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = set()\n",
    "    embd = {}\n",
    "    word = None\n",
    "    with open(filename) as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                row = line.strip().split(' ')\n",
    "                word = row[0]\n",
    "                vocab.add(row[0])\n",
    "                embd[word] = np.array([float(x) for x in row[1:]])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(word)\n",
    "    print('Loaded GloVe!')\n",
    "    return vocab,embd\n",
    "vocab_set,embd = loadGloVe(filename)\n",
    "# vocab_size = len(vocab)\n",
    "# vocab_set = set(vocab)\n",
    "# embedding_dim = len(embd[0])\n",
    "# embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_set.add('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd['<UNK>'] = np.zeros(50, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = list(vocab_set)\n",
    "word_to_index = {vocab_list[i]: i for i in range(len(vocab_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('<UNK>' in vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import numpy\n",
    "# def generate_vocabulary():\n",
    "#     i = 0\n",
    "#     vocab = set() \n",
    "#     with open(\"tip.json\",encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             tip = json.loads(line.strip())['text']\n",
    "# #             print (tip)\n",
    "#             words = tip.lower().split()\n",
    "#             vocab |= set(words)\n",
    "#             i +=1\n",
    "#             if i == 5000:\n",
    "#                 break\n",
    "#     vocab.add('start')\n",
    "    \n",
    "#     return vocab,list(vocab)\n",
    "# # def generate_data():\n",
    "# vocab_set,vocab_list = generate_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_index = {vocab_list[i]:i for i in range(len(vocab_list))}\n",
    "transnet_size = 64\n",
    "sequence_length = 3\n",
    "def one_hot(word,word_index,vocab_len):\n",
    "    x = [0]*vocab_len\n",
    "    if word in word_index:\n",
    "        x[word_index[word]] = 1\n",
    "    return x\n",
    "# def one_hot_(k,l):\n",
    "#     x = [0]*(l)\n",
    "#     while k > 0:        \n",
    "#         x[k%l] = 1\n",
    "#         k = k//l\n",
    "#     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def generate_train():\n",
    "#     j = 0\n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     total = 1000\n",
    "#     tnet_file = open('../src/ex2_data/train_epochs/newfile.txt')\n",
    "#     with open(\"./data/train_lex_data.txt\") as f:\n",
    "#         for line in f:\n",
    "#             user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "#             #vec = one_hot_(j,transnet_size)\n",
    "#             words = ['start' for _ in range(sequence_length)] + tip.strip().lower().split()\n",
    "#             vectors = []\n",
    "#             onehot = []\n",
    "#             for word in words:\n",
    "#                 encoding = one_hot(word)\n",
    "#                 onehot += [encoding]\n",
    "#                 vectors += [encoding+vec]\n",
    "#             for i in range(sequence_length-1,len(words)-1):\n",
    "#                 X += [vectors[i-sequence_length+1:i+1]]\n",
    "#                 Y += [onehot[i+1]]\n",
    "#             j += 1\n",
    "#             if j % 100 == 0:\n",
    "#                 print(j)\n",
    "#             if j == total:\n",
    "#                 break\n",
    "#     return np.array(X),np.array(Y)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X,Y =generate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punc = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_item_index = pickle.load(open('user_item_index_dict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_item_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def new_generate_train():\n",
    "    sequence_length = 3\n",
    "    j = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    total = 500\n",
    "    tnet_file = pickle.load(open('./gen_review_vec_train.pkl','rb'))\n",
    "    with open(\"./data/train.txt\") as f:\n",
    "        for line in f:\n",
    "#             rev_emb = np.zeros(50)\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            rev_index = user_item_index[(user_id,item_id)]\n",
    "            \n",
    "            #vec = one_hot_(j,transnet_size)\n",
    "            rev_words = [w.lower() for w in tip.split()]\n",
    "            words = []\n",
    "            for w in rev_words:\n",
    "                prev = 0\n",
    "                for ch in range(len(w)):\n",
    "                    if w[ch] in punc:\n",
    "                        if prev < ch:\n",
    "                            words += [w[prev:ch]]\n",
    "                        words += [w[ch]]\n",
    "                        prev = ch + 1\n",
    "                if prev < len(w):\n",
    "                    words += [w[prev:]]\n",
    "            if len(words)< 5:\n",
    "                words += ['<UNK>']*(5-len(words))\n",
    "            elif len(words) > 5:\n",
    "                words = words[:5]\n",
    "            words = ['<UNK>' for _ in range(sequence_length)] + words\n",
    "            vectors = []\n",
    "            onehot = []\n",
    "            vec = tnet_file[rev_index] #index of review from tnet file\n",
    "            j += 1\n",
    "#             if j % 10 == 0:\n",
    "#                 print(j,end=\" \")\n",
    "            for word in words:\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                onehot += [onehot_encoding]\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "            X = []\n",
    "            Y = []\n",
    "            for i in range(sequence_length-1,len(words)-1):\n",
    "                X += [vectors[i-sequence_length+1:i+1]]\n",
    "                Y += [onehot[i+1]]\n",
    "#             X,Y = np.array(X),np.array(Y)\n",
    "#             print(X.shape,Y.shape)\n",
    "            yield X,Y\n",
    "#                 Y += []\n",
    "            \n",
    "#             if j == total:\n",
    "#                 break\n",
    "#     return X,Y#np.array(X),np.array(Y)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X =[]\n",
    "Y = []\n",
    "for x,y in new_generate_train():\n",
    "    X += x\n",
    "    Y += y\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 3, 114)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 400001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for x,y in new_generate_train():\n",
    "#     if i % 10 == 0:\n",
    "#         print(i,end=\" \")\n",
    "#     i += 1\n",
    "#     if i == 600:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 3\n",
    "transnet_size = 64\n",
    "word_emb_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(vocab_list)\n",
    "model = Sequential()\n",
    "model.add(LSTM(15,input_shape=(sequence_length,word_emb_size+transnet_size)))\n",
    "model.add(Dense(V,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, Y_train = a,b #generate_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maverick/anaconda/lib/python2.7/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2500/2500 [==============================] - 109s 44ms/step - loss: 9.0502 - acc: 0.0324\n",
      "Epoch 2/20\n",
      "2500/2500 [==============================] - 88s 35ms/step - loss: 6.3865 - acc: 0.0340\n",
      "Epoch 3/20\n",
      "2500/2500 [==============================] - 91s 37ms/step - loss: 6.0082 - acc: 0.0420\n",
      "Epoch 4/20\n",
      "2500/2500 [==============================] - 97s 39ms/step - loss: 5.7802 - acc: 0.0480\n",
      "Epoch 5/20\n",
      "2500/2500 [==============================] - 93s 37ms/step - loss: 5.5432 - acc: 0.0620\n",
      "Epoch 6/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 5.3282 - acc: 0.0716\n",
      "Epoch 7/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 5.1502 - acc: 0.0832\n",
      "Epoch 8/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 5.0062 - acc: 0.0936\n",
      "Epoch 9/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.8808 - acc: 0.1032\n",
      "Epoch 10/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.7832 - acc: 0.0996\n",
      "Epoch 11/20\n",
      "2500/2500 [==============================] - 86s 34ms/step - loss: 4.6904 - acc: 0.1104\n",
      "Epoch 12/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.6019 - acc: 0.1128\n",
      "Epoch 13/20\n",
      "2500/2500 [==============================] - 86s 35ms/step - loss: 4.5157 - acc: 0.1168\n",
      "Epoch 14/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.4432 - acc: 0.1284\n",
      "Epoch 15/20\n",
      "2500/2500 [==============================] - 86s 34ms/step - loss: 4.3820 - acc: 0.1256\n",
      "Epoch 16/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.3201 - acc: 0.1368\n",
      "Epoch 17/20\n",
      "2500/2500 [==============================] - 86s 35ms/step - loss: 4.2882 - acc: 0.1364\n",
      "Epoch 18/20\n",
      "2500/2500 [==============================] - 86s 35ms/step - loss: 4.2435 - acc: 0.1488\n",
      "Epoch 19/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.2051 - acc: 0.1536\n",
      "Epoch 20/20\n",
      "2500/2500 [==============================] - 87s 35ms/step - loss: 4.1672 - acc: 0.1592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122346150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "model.fit(X,Y,nb_epoch=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#sample\n",
    "import numpy as np\n",
    "def long_to_small(preds):\n",
    "    preds = preds.tolist()\n",
    "    index = []\n",
    "    small_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] > 0.001:\n",
    "            small_preds += [preds[i]]\n",
    "            index += [i]\n",
    "    return small_preds,index\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    #print preds[:10], np.max(preds)\n",
    "    #print(len(preds))\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_test,Y_test = generate_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re  = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(re)):\n",
    "#     print(vocab_list[sample(re[i])],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def test(out):\n",
    "    sequence_length = 3\n",
    "    \n",
    "    j = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    #total = 1000\n",
    "    tnet_file = pickle.load(open('./gen_review_vec_test.pkl','rb')) #changr to test\n",
    "    with open(\"./data/Old/test_lex_data.txt\") as f:\n",
    "        for line in f:\n",
    "            rev_emb = np.zeros(50)\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            #vec = one_hot_(j,transnet_size)\n",
    "#             rev_words = [w.lower() for w in review_text.split()]\n",
    "            words = []\n",
    "            words = ['<UNK>' for _ in range(sequence_length)] + words\n",
    "            vectors = []\n",
    "            onehot = []\n",
    "            vec = tnet_file[j]\n",
    "            for word in words:\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                onehot += [onehot_encoding]\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "            #for i in range(sequence_length-1,len(words)-1):\n",
    "            X += [vectors[0:sequence_length]]\n",
    "#                 Y += [onehot[i+1]]\n",
    "            j += 1\n",
    "            yp = model.predict(np.array(X))[0]\n",
    "            yp_small, index = long_to_small(yp)\n",
    "            yp = vocab_list[index[sample(yp_small)]]\n",
    "            generated_tip = [yp]\n",
    "            words += [yp]\n",
    "            for i in range(sequence_length,10+sequence_length):\n",
    "                word = generated_tip[-1]\n",
    "                onehot_encoding = one_hot(word,word_to_index,len(vocab_set))\n",
    "                onehot += [onehot_encoding]\n",
    "                if word in embd:\n",
    "                    encoding = embd[word]\n",
    "                else:\n",
    "                    encoding = embd['<UNK>']\n",
    "                vectors += [np.concatenate([encoding,vec])]\n",
    "                X += [vectors[i-sequence_length+1:i+1]]\n",
    "                x = [X[-1]]\n",
    "                yp = model.predict(np.array(x))[0]\n",
    "                yp_small, index = long_to_small(yp)\n",
    "                yp = vocab_list[index[sample(yp_small)]]\n",
    "                generated_tip += [yp]\n",
    "                words += [yp]\n",
    "            out += [\" \".join(generated_tip)]\n",
    "            if j == 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a a really hair 9 for call . here . is',\n",
       " \"for can a me . right . ' to sure in\",\n",
       " '. prepared place good clean ! inside pizza around and have',\n",
       " 'a with . been down ! ! spicy like salad for',\n",
       " \"my the but ' s better ! will you were the\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_data():\n",
    "    with open('tip.json') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            tip = json.loads(line.strip())['text']\n",
    "            vec = one_hot_(i,transnet_size)\n",
    "            X = []\n",
    "            one_hot_v = one_hot('start')\n",
    "            X = [one_hot_v+vec for _ in range(sequence_length)]\n",
    "                \n",
    "            return vec,tip,numpy.array(X).reshape(1,sequence_length,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec,tip,X = generate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the kidding. has even this up be people to easy "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y = model.predict(X)\n",
    "#     print(y.shape)\n",
    "#     print (y)\n",
    "    w = sample(y[0])\n",
    "#     print(w)\n",
    "    print(vocab_list[w],end=\" \")\n",
    "    X = numpy.delete(X,0,1)\n",
    "#     print(X.shape)\n",
    "    X = numpy.insert(X,sequence_length-1,one_hot_(w,len(vocab_list))+vec,axis=1)\n",
    "#     print(X.shape)\n",
    "#     print(X[0][sequence_length-1])\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get here early enough to have dinner.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0677953 ,  0.12908284,  0.00995188, ...,  0.09868402,\n",
       "         0.05035567,  0.0261237 ],\n",
       "       [ 0.0677953 ,  0.12908284,  0.00995188, ...,  0.09868402,\n",
       "         0.05035567,  0.0261237 ],\n",
       "       [ 0.0677953 ,  0.12908284,  0.00995188, ...,  0.09868402,\n",
       "         0.05035567,  0.0261237 ],\n",
       "       ..., \n",
       "       [ 0.06633914,  0.1292228 ,  0.00890549, ...,  0.09720803,\n",
       "         0.04496233,  0.02088072],\n",
       "       [ 0.07077023,  0.13585889,  0.01115206, ...,  0.09501065,\n",
       "         0.03852132,  0.01822961],\n",
       "       [ 0.06911414,  0.13658382,  0.01111554, ...,  0.09772882,\n",
       "         0.04012528,  0.02351698]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('./gen_review_vec_train.pkl','rb'),encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open ('data/train.txt','w',encoding='utf-8')\n",
    "i = 0\n",
    "with open(\"./data/train_lex_data.txt\",encoding='utf-8') as f:\n",
    "        for line    in f:\n",
    "            user_id,item_id,rating,review_id,review_text,tip = line.strip().split('\\t')\n",
    "            if len(tip) < 25:\n",
    "                continue\n",
    "            file.write(line)\n",
    "            i += 1\n",
    "            if i == 500:\n",
    "                break\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
